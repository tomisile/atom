{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddbf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting sb today scraper...\n",
      "Found 47 events for today\n",
      "\n",
      "üìä Summary:\n",
      "   - Total events found: 47\n",
      "üíæ Data saved to sb_today_02-09-25-19-37-29.csv\n",
      "‚úÖ Successfully scraped and saved 47 matches to sb_today_02-09-25-19-37-29.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def get_random_headers():\n",
    "    \"\"\"Load and return a random set of headers from the JSON file.\"\"\"\n",
    "    # Get the directory where the current script is located\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    headers_file = os.path.join(script_dir, 'browser_headers.json')\n",
    "    \n",
    "    try:\n",
    "        with open(headers_file, 'r') as f:\n",
    "            headers_list = json.load(f)\n",
    "        \n",
    "        # Return a random set of headers\n",
    "        return random.choice(headers_list)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        # Fallback to your original headers if file not found\n",
    "        return {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "\n",
    "\n",
    "def scrape_sb_today():\n",
    "    \"\"\"\n",
    "    Scrapes SportyBet today's football matches and extracts match data\n",
    "    Returns a list of dictionaries containing match data\n",
    "    \"\"\"\n",
    "    url = \"https://www.sportybet.com/ng/sport/football/today\"\n",
    "    \n",
    "    # Headers to mimic a real browser\n",
    "    # headers = get_random_headers()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Set up headless Chrome\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")  # Run without opening a browser window\n",
    "        chrome_options.add_argument(\"--no-sandbox\")  # For stability in some environments\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Avoid resource issues\n",
    "        chrome_options.add_argument(\"--disable-gpu\")  # Additional stability\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        chrome_options.add_argument(\"--disable-logging\")  # Reduce log noise\n",
    "        chrome_options.add_argument(\"--log-level=3\")  # Only fatal errors\n",
    "        chrome_options.add_argument(f\"user-agent={headers['User-Agent']}\")  # Reuse your user-agent for consistency\n",
    "        \n",
    "        # Initialize driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for JS to load (adjust timeout if needed; 10 seconds should suffice for this site)\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        # Get page source and clean it before parsing\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "        # Clean the page source to remove any problematic content\n",
    "        # Remove any WebDriver-related paths that might be causing issues\n",
    "        page_source = re.sub(r'/[^<>]*?\\.wdm/[^<>]*?chromedriver[^<>]*?', '', page_source)\n",
    "        page_source = re.sub(r'\\[[^<>\\[\\]]*?chromedriver[^<>\\[\\]]*?\\]', '', page_source)\n",
    "\n",
    "        # Parse with explicit parser and error handling\n",
    "        try:\n",
    "            # Try html.parser first (most robust)\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        except Exception as e1:\n",
    "            print(f\"‚ö†Ô∏è html.parser failed: {e1}\")\n",
    "            try:\n",
    "                # Fallback to lxml if available\n",
    "                soup = BeautifulSoup(page_source, 'lxml')\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ö†Ô∏è lxml parser failed: {e2}\")\n",
    "                # Last resort - use html5lib if available\n",
    "                try:\n",
    "                    soup = BeautifulSoup(page_source, 'html5lib')\n",
    "                except Exception as e3:\n",
    "                    print(f\"‚ùå All parsers failed. html5lib error: {e3}\")\n",
    "                    driver.quit()\n",
    "                    return []\n",
    "        \n",
    "        # Find all matches with the correct class structure\n",
    "        matches = soup.find_all('div', class_='m-table-row m-content-row match-row')\n",
    "        print(f\"Found {len(matches)} events for today\")\n",
    "        \n",
    "        extracted_data = []\n",
    "        \n",
    "        for match in matches:\n",
    "            try:                \n",
    "                left_team_cell = match.find(class_='m-table-cell left-team-cell')\n",
    "\n",
    "                if left_team_cell:\n",
    "                    left_team_table = left_team_cell.find(class_='left-team-table')\n",
    "                    if left_team_table:\n",
    "                        game_id_elem = left_team_table.find(class_='game-id')\n",
    "                        if game_id_elem:\n",
    "                            game_id_text = game_id_elem.get_text(strip=True)\n",
    "                            # Extract 5-digit number using regex\n",
    "                            game_id_match = re.search(r'\\b\\d{5}\\b', game_id_text)\n",
    "                            # if game_id_match:\n",
    "                            #     match_data['game_id'] = game_id_match.group()\n",
    "                            # else:\n",
    "                            #     match_data['game_id'] = game_id_text  # Fallback to full text if no 5-digit found\n",
    "\n",
    "                        # Extract time\n",
    "                        time_elem = left_team_table.find(class_='clock-time')\n",
    "                        if time_elem:\n",
    "                            time_text = time_elem.get_text(strip=True)\n",
    "        \n",
    "                # Find teams container\n",
    "                teams_container = match.find(class_='teams')\n",
    "                if not teams_container:\n",
    "                    continue\n",
    "                \n",
    "                # Extract team names\n",
    "                home_team_elem = teams_container.find(class_='home-team')\n",
    "                away_team_elem = teams_container.find(class_='away-team')\n",
    "                \n",
    "                if not home_team_elem or not away_team_elem:\n",
    "                    continue\n",
    "                \n",
    "                home_team = home_team_elem.get_text(strip=True)\n",
    "                away_team = away_team_elem.get_text(strip=True)\n",
    "                \n",
    "                # Extract title from teams container\n",
    "                title = teams_container.get('title', f\"{home_team} vs {away_team}\")\n",
    "                   \n",
    "                match_data = {\n",
    "                    'time': time_text,\n",
    "                    'title': title,\n",
    "                    'game-id': game_id_match.group() if game_id_match else game_id_text,\n",
    "                    'home-team': home_team,\n",
    "                    'away-team': away_team,\n",
    "                }\n",
    "                extracted_data.append(match_data)                \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing match: {e}\")\n",
    "                continue\n",
    "\n",
    "        driver.quit()  # Clean up browser session\n",
    "        \n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"   - Total events found: {len(matches)}\")\n",
    "        \n",
    "        return extracted_data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching data: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename=None):\n",
    "    \"\"\"\n",
    "    Save extracted data to CSV file with timestamp\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        print(\"‚ùå No data to save\")\n",
    "        return False, None\n",
    "    \n",
    "    if filename is None:\n",
    "        # Generate filename with current timestamp\n",
    "        current_time = datetime.now()\n",
    "        filename = f\"sb_save_{current_time.strftime('%d-%m-%y-%H-%M-%S')}.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"üíæ Data saved to {filename}\")\n",
    "        return True, filename\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving to CSV: {e}\")\n",
    "        return False, None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the scraper and save data\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting sb today scraper...\")\n",
    "    \n",
    "    # Scrape the data\n",
    "    matches_data = scrape_sb_today()\n",
    "    \n",
    "    if matches_data:\n",
    "        # Save to CSV\n",
    "        success, filename = save_to_csv(matches_data)\n",
    "        \n",
    "        if success:\n",
    "            print(f\"‚úÖ Successfully scraped and saved {len(matches_data)} matches to {filename}\")\n",
    "        else:\n",
    "            print(\"‚ùå Failed to save data to CSV\")\n",
    "    else:\n",
    "        print(\"‚ùå No data was scraped\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
